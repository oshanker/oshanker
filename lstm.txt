https://www.worldscientific.com/doi/abs/10.1142/S1793830921501524

 Discrete Mathematics, Algorithms and Applications
 Vol. 14, No. 05, 2150152 (2022) 
 
Deep learning-based approximation of Goldbach partition function

Eunmi Kim
https://doi.org/10.1142/S1793830921501524

Goldbach’s conjecture is one of the oldest and famous unproved problems in number theory. 
Using a deep learning model, we obtain an approximation of the Goldbach partition function, 
which counts the number of ways of representing an even number greater than 4 as 
a sum of two primes. We use residues of number modulo first 25 primes as features 
and archive a 
more accurate approximation, which reduces error rate from 3.0% to 0.6%.

https://www.worldscientific.com/page/dmaa/submission-guidelines 

https://www.elsevier.com/journals/journal-of-symbolic-computation/0747-7171/open-access-options




Discrete Mathematics and Applications Volume 20 Issue 4
https://www.degruyter.com › journal › key › dma › html
Volume 20, issue 4 of the journal Discrete Mathematics and Applications was ... 
On large distances between neighbouring zeros of the Riemann zeta-function.

Discrete Mathematics and Applications Volume 22 Issue 5-6
https://www.degruyter.com › journal › key › dma › html
Volume 22, issue 5-6 of the journal Discrete Mathematics and Applications was ... 
containing the ordinates of successive zeros of the Riemann zeta function.

https://www.degruyter.com/journal/key/dma/20/4/html?lang=en
 Discrete Mathematics and Applications
 The journal makes no page charges
 https://www.degruyter.com/journal/key/dma/html#submit
 
 Submission
Submission is done electronically by emailing both a PDF file 
and the source files (LaTeX) to the Editorial Office at dma@mi.ras.ru

Yuri I. Matiyasevich; St. Petersburg


        print(
            """
977/977 [==============================] - 23s 23ms/step - loss: 9.8978 - mae: 1.4766 - val_loss: 9.8637 - val_mae: 1.5293
Epoch 20/20
977/977 [==============================] - 23s 24ms/step - loss: 9.6790 - mae: 1.4464 - val_loss: 9.7542 - val_mae: 1.3489
489/489 [==============================] - 7s 12ms/step - loss: 9.3265 - mae: 1.3530
Test MAE: 1.35    
      
https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/     

this post takes a look at another problem that plagues training of neural networks, 
pathological curvature.

While local minima and saddle points can stall our training, 
pathological curvature can slow down training to an extent that 
the machine learning practitioner might think that search 
has converged to a sub-optimal minma. 
 pathological curvature 

Adam or Adaptive Moment Optimization algorithms combines the heuristics of 
both Momentum and RMSProp. however, ...

https://www.tensorflow.org/api_docs/python/tf/keras/optimizers 
class Adadelta: Optimizer that implements the Adadelta algorithm.

class Adagrad: Optimizer that implements the Adagrad algorithm.

class Adam: Optimizer that implements the Adam algorithm.

class Adamax: Optimizer that implements the Adamax algorithm.

class Ftrl: Optimizer that implements the FTRL algorithm.

class Nadam: Optimizer that implements the NAdam algorithm.

class Optimizer: Base class for Keras optimizers.

class RMSprop: Optimizer that implements the RMSprop algorithm.

class SGD: Gradient descent (with momentum) optimizer.

"""
            )
